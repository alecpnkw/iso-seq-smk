from snakemake.utils import min_version
import pandas as pd
from os.path import dirname, basename
min_version("6.0")

configfile: "config/config.yaml"

samples = pd.read_csv(config["samples"], sep = '\t').set_index("sample", drop=False)

localrules: all, cDNA_cupcake_init

# targeting...
rule all:
    input:
        #outputs from TALON... 
        expand("results/talon/{dataset}_talon.gtf", dataset = samples["dataset"]),
        expand("results/talon/{dataset}_talon_abundance_filtered.tsv", dataset = samples["dataset"]),
        expand("results/sqanti3/{dataset}/{dataset}_corrected.gtf", dataset = samples["dataset"]),
        "results/multiqc/multiqc-report.html"

# Todo: add intial PacBio processing...

def INPUT_BAM(wc):
    return samples.loc[wc.sample, "input_bam"]

# FLNC .bam to .fastq
rule flnc_to_fastq:
    input: INPUT_BAM
    output: "results/flnc_fastq/{sample}.fastq.gz"
    #conda:
    #    "envs/samtools.yaml"
    envmodules:
        "samtools/1.11"
    shell:
        """
        samtools view {input} | \
        awk '{{printf("@%s\\n%s\\n+\\n%s\\n", $1, $10, $11)}}' | \
        gzip > {output}
        """
        
# Todo: add minimap2 conda env
rule minimap2_sam:
    input:
        target=config["reference"], # can be either genome index or genome fasta
        query="results/flnc_fastq/{sample}.fastq.gz"
    output:
        "results/minimap2/{sample}.sam" # can add additional output files
    params:
        output_prefix = lambda wc, output: output[0].split(".sam")[0],
        threads = 6
    threads: 6
    resources:
        mem_mb = 6000 #change to mem_mb
    envmodules:
        "samtools/1.9",
        "minimap2/2.17"
    shell:
        """
        minimap2 -ax splice -uf --secondary=no -C5 -t {params.threads} --MD {input.target} {input.query} | \
        samtools sort -O sam -o {output} && \
        samtools flagstat {output} > {params.output_prefix}.flagstat && \
        samtools idxstats {output} > {params.output_prefix}.idxstats && \
        samtools stats {output} > {params.output_prefix}.stats && \
        samtools view {output} | cut -f1,5 > {params.output_prefix}.mapq
        """

rule filter_sam:
    input:
        "results/minimap2/{sample}.sam"
    output:
        "results/minimap2_filt/{sample}.sam"
    threads:
        5
    params:
        output_prefix = lambda wc, output: output[0].split('.sam')[0],
        threads = 30
    #conda:
    #    "envs/sambamba.yaml"
    envmodules:
        "samtools/1.9",
        "sambamba/0.5.6"
    shell:
        """
        sambamba view \
        --with-header \
        --nthreads {params.threads} \
        --sam-input \
        --format sam \
        --filter "not unmapped and mapping_quality >= 50" \
        {input} | \
        samtools sort -O sam -o {output} && \
        samtools flagstat {output} > {params.output_prefix}.flagstat && \
        samtools idxstats {output} > {params.output_prefix}.idxstats && \
        samtools stats {output} > {params.output_prefix}.stats && \
        samtools view {output} | cut -f1,5 > {params.output_prefix}.mapq
        """

# decide how to deal with these...
# ask denis for the R script that runs this!
#From the documentation I think concatenation would also work...
rule merge_sjs:
    input:  
        config["short_read_splice_junctions"]
    output:
        "results/merge_sjs/SJ.merged.tab"
    shell:
        "cat {input} > {output}"

# this looks like it pulls the first field of the chr names...
rule rename_fasta_chromosomes:
    input: config["reference"]
    output: "results/renamed_ref/{dataset}_renamed.fa"
    shell:
        """
        cut -d ' ' -f1 {input} > {output}
        """

# start of TranscriptClean
# perhaps add acondition to filter based on short-read splice junctions...
# one .sam at a time
rule transcript_clean:
    input:
        "results/minimap2_filt/{sample}.sam",
        "results/renamed_ref/{dataset}_renamed.fa", #genome
        "results/merge_sjs/SJ.merged.tab"  #merged SJs
    output:
        multiext("results/transcript_clean/{dataset}/{sample}/TC_clean", ".sam", ".log", ".fa", ".TE.log")
    params:
        output_prefix = lambda wc, output: dirname(output[0]),
        tc_exec = config["transcriptclean_path"],
        tempdir = "tmp/{dataset}_{sample}/"
    threads:
        10
    resources:
        mem_mb = 8000
    conda: 
        "envs/transcript_clean.yaml"
    shell:
        """
        python3  {params.tc_exec} \
        --sam {input[0]} \
        --genome {input[1]} \
        --spliceJns {input[2]} \
        --tmpDir {params.tempdir} \
        --threads {threads} \
        --deleteTmp \
        --canonOnly \
        --primaryOnly \
        --outprefix {params.output_prefix}
        """

# initialize db (uses SQLite)...
# follow up with Denis about additional options...
rule talon_initialize:
    input:
        config["annotation"] #pass in via config
    output:
        "results/talon/{dataset}_gene_models.db", #only one genemodels.db per run...
    conda:
        "envs/talon.yaml"
    params:
        annot = basename(config["annotation"]),
        output_prefix = lambda wildcards, output: output[0].split('.db')[0]
    threads:
        4
    resources:
        mem_mb = 6000 # change to mem_mb
    shell: 
        """
        talon_initialize_database \
        --f {input} \
        --a {params.annot} \
        --g {wildcards.dataset} \
        --o {params.output_prefix}
        """

#label/flag reads...
rule talon_label:
    input:
        "results/transcript_clean/{dataset}/{sample}/TC_clean.sam" #aligned, filtered reads
    output:
        "results/talon_label/{dataset}/{sample}_labeled.sam",
        "results/talon_label/{dataset}/{sample}_read_labels.tsv"
    conda:
        "envs/talon.yaml"
    threads: 2
    params:
        output_prefix = lambda wc, output: output[0].split('_labeled.sam')[0],
        ref = config["reference"],
        ar = 20,
        tempdir = "tmp/talon/{sample}/"
    shell:
        """
        talon_label_reads \
        --f {input} \
        --g {params.ref}  \
        --t {threads} \
        --ar {params.ar} \
        --tmpDir {params.tempdir} \
        --deleteTmp \
        --o {params.output_prefix}
        """

#writes a new config.csv... 
#tweak this... 
rule create_talon_config:
    input:
        # all labeled files as input... 
        expand("results/talon_label/{dataset}/{sample}_labeled.sam", zip, sample = samples.index, dataset = samples.dataset)
    params:
        config = config["samples"]
    output:
        "results/talon/{dataset}_config.csv"
    run:
        import pandas as pd
        tbl = pd.read_csv(params.config, sep = '\t')
        tbl["file"] = input
        keeps = tbl.dataset == wildcards.dataset
        cols = ["sample", "dataset", "source", "file"]
        tbl.loc[keeps, cols].to_csv(output[0], header = False, index = False)

#TALON... 
rule talon:
    input:
        config = "results/talon/{dataset}_config.csv",
        db = "results/talon/{dataset}_gene_models.db"
    params: 
        build = basename(config["reference"])
    threads:
        10
    resources:
        mem_mb = 10000
    output:
        "results/talon/{dataset}_QC.log"
    conda:
        "envs/talon.yaml"
    shell: 
        """
        talon \
        --f {input.config} \
        --db {input.db} \
        --build {wildcards.dataset} \
        --threads {threads} \
        --cov 0.99 \
        --identity 0.95 \
        --o results/talon/{wildcards.dataset}
        """

# transcript model filtering... 
# dataset names need to match talon config...
# this differs from Denis's run... which supplies an SQL query
rule talon_filter:
    input:
        "results/talon/{dataset}_gene_models.db",
        "results/talon/{dataset}_QC.log"
    output:
        "results/talon/{dataset}_filtered_transcripts.csv"
    params:
        samples = lambda wc: ",".join(samples.index[samples.dataset == wc.dataset]),
        maxFracA = 0.5,
        minCount = 0,
        minDatasets = 0,
        annot = basename(config["annotation"])
    threads:
        4
    resources:
        mem_mb = 5000 
    conda:
        "envs/talon.yaml"
    shell:
        """
        talon_filter_transcripts \
        --db {input[0]} \
        --datasets {params.samples} \
        -a {params.annot} \
        --maxFracA {params.maxFracA} \
        --minCount {params.minCount} \
        --minDatasets {params.minDatasets} \
        --o {output}
        """

#tabulate
rule talon_abundance:
    input:
        "results/talon/{dataset}_gene_models.db",
        "results/talon/{dataset}_filtered_transcripts.csv"
    output: 
        "results/talon/{dataset}_talon_abundance_filtered.tsv"
    conda:
        "envs/talon.yaml"
    threads:
        4
    resources:
        mem_mb = 5000
    params:
        annot = basename(config["annotation"]),
        output_prefix = lambda wc, output: output[0].split("_talon_abundance_filtered.tsv")[0]
    shell:
        """
        talon_abundance \
        --db {input[0]} \
        --whitelist {input[1]} \
        --annot {params.annot} \
        --build {wildcards.dataset} \
        --o {params.output_prefix}
        """

#GTF
rule talon_GTF:
    input:
        "results/talon/{dataset}_gene_models.db",
        "results/talon/{dataset}_filtered_transcripts.csv"
    output:
        "results/talon/{dataset}_talon.gtf"
    conda:
        "envs/talon.yaml"
    params:
        annot = basename(config["annotation"]),
        output_prefix = lambda wc, output: output[0][:-10]
    threads:
        4
    resources:
        mem_mb = 5000
    shell: 
        """
        talon_create_GTF \
        --db {input[0]} \
        --whitelist {input[1]} \`
        -a {params.annot} \
        --build {wildcards.dataset} \
        --o {params.output_prefix}
        """

# still needs work... 
rule cDNA_cupcake_init:
    output: "logs/cDNA_cupcake_init.success.log"
    params: 
        path = config["cDNA_cupcake_path"]
    conda: "envs/sqanti3.yaml"
    shell: 
        """
        WDIR=pwd
        cd {params.path}
        python {params.path}/setup.py build
        python {params.path}/setup.py install
        touch $"{WDIR}"{output}
        """

rule sqanti_qc:
    input:
        "logs/cDNA_cupcake_init.success.log",
        isoforms = "results/talon/{dataset}_talon.gtf",
        annot = config["annotation"],
        genome = "results/renamed_ref/{dataset}_renamed.fa" # pass in renamed chr for consistency
    output:
        "results/sqanti3/{dataset}/{dataset}_corrected.gtf"
    conda:
        "envs/sqanti3.yaml"
    params:
        addtl_opt = "--force_id_ignore --skipORF --report both ",
        sj_files = config["short_read_splice_junctions"],
        out_dir = lambda wc, output: dirname(output[0]),
        cage_path = config["CAGE_peaks"],
        polya_motifs = config["polyA_motifs"]
    threads: 12
    resources:
        mem_mb = 8000
    shell:
        """
        python workflow/scripts/sqanti3_qc.py \
        {input.isoforms} {input.annot} {input.genome} \
        --cage_peak CAGE_PEAK \
        --polyA_motif_list POLYA_MOTIF_LIST \
        -t {threads} \
        -n {threads} \
        -d {params.out_dir} \
        -o {wildcards.dataset} \
        -c {params.sj_files} \
        {params.addtl_opt}\
        """

rule multiqc:
    input:
        expand("results/talon/{dataset}_talon.gtf", dataset = samples["dataset"]),
        expand("results/talon/{dataset}_talon_abundance_filtered.tsv", dataset = samples["dataset"]),
    output:
        "results/multiqc/multiqc-report.html"
    conda: 
        "envs/multiqc.yaml"
    shell:
        """
        multiqc \
        --no-data-dir \
        --filename {output} \
        results/
        """