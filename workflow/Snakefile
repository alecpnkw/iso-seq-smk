from snakemake.utils import min_version
import pandas as pd
from os.path import dirname, basename
min_version("6.0")

configfile: "config/config.yaml"

samples = pd.read_csv(config["samples"], sep = ',').set_index("sample", drop=False)

# targeting...
rule all:
    input:
        #outputs from TALON... 
        expand("results/talon/{dataset}_talon.gtf", dataset = samples["dataset"]),
        expand("results/talon/{dataset}_talon_abundance_filtered.tsv", dataset = samples["dataset"]),

# Todo: add intial PacBio processing...

def INPUT_BAM(wc):
    return samples.loc[wc.sample, "input_bam"]

# FLNC .bam to .fastq
rule flnc_to_fastq:
    input: INPUT_BAM
    output: "results/flnc_fastq/{sample}.fastq.gz"
    #conda:
    #    "envs/samtools.yaml"
    envmodules:
        "samtools/1.11"
    shell:
        """
        samtools view {input} | \
        awk '{{printf("@%s\\n%s\\n+\\n%s\\n", $1, $10, $11)}}' | \
        gzip > {output}
        """

rule minimap2_sam:
    input:
        target=config["reference"], # can be either genome index or genome fasta
        query="results/flnc_fastq/{sample}.fastq.gz"
    output:
        "results/minimap2/{sample}.sam" # can add additional output files
    params:
        output_prefix = lambda wc, output: output[0].split(".sam")[0],
        threads = 6
    resources:
        mem_mb = 10000
    threads: 6
    resources:
        memory = 6000
    envmodules:
        "samtools/1.9",
        "minimap2/2.17"
    shell:
        """
        minimap2 -ax splice -uf --secondary=no -C5 -t {params.threads} --MD {input.target} {input.query} | \
        samtools sort -O sam -o {output} && \
        samtools flagstat {output} > {params.output_prefix}.flagstat && \
        samtools idxstats {output} > {params.output_prefix}.idxstats && \
        samtools stats {output} > {params.output_prefix}.stats && \
        samtools view {output} | cut -f1,5 > {params.output_prefix}.mapq
        """

rule filter_sam:
    input:
        "results/minimap2/{sample}.sam"
    output:
        "results/minimap2_filt/{sample}.sam"
    threads:
        5
    params:
        output_prefix = lambda wc, output: output[0].split('.sam')[0],
        threads = 30
    #conda:
    #    "envs/sambamba.yaml"
    envmodules:
        "samtools/1.9",
        "sambamba/0.5.6"
    shell:
        """
        sambamba view \
        --with-header \
        --nthreads {params.threads} \
        --sam-input \
        --format sam \
        --filter "not unmapped and mapping quality >= 50" \
        {input} | \
        samtools sort -O sam -o {output} && \
        samtools flagstat {output} > {params.output_prefix}.flagstat && \
        samtools idxstats {output} > {params.output_prefix}.idxstats && \
        samtools stats {output} > {params.output_prefix}.stats && \
        samtools view {output} | cut -f1,5 > {params.output_prefix}.mapq
        """

# decide how to deal with these...
# ask denis for the R script that runs this!
#From the documentation I think concatenation would also work...
rule merge_sjs:
    input:  
        config["short_read_splice_junctions"]
    output:
        "results/merge_sjs/SJ.merged.tab"
    shell:
        "cat {input} > {output}"

# this looks like it pulls the first field of the chr names...
rule rename_fasta_chromosomes:
    input: config["reference"]
    output: "results/renamed_ref/{dataset}_renamed.fa"
    shell:
        """
        cut -d ' ' -f1 {input} > {output}
        """

# start of TranscriptClean
# perhaps add acondition to filter based on short-read splice junctions...
# one .sam at a time
rule transcript_clean:
    input:
        "results/minimap2_filt/{sample}.sam",
        "results/renamed_ref/{dataset}_renamed.fa", #genome
        "results/merge_sjs/SJ.merged.tab"  #merged SJs
    output:
        "results/transcript_clean/{dataset}/{sample}_clean.sam" # Todo: figure out what this output is...
    params:
        output_prefix = lambda wc, output: dirname(output[0]),
        tc_exec = config["transcriptclean_path"],
        tempdir = "tmp/{dataset}_{sample}"
    conda: 
        "envs/transcript_clean.yaml"
    shell:
        """
        python3  {params.tc_exec} \
        --sam {input[0]} \
        --genome {input[1]} \
        --spliceJns {input[2]} \
        --tmpDir {params.tempdir} \
        --threads {threads} \
        --deleteTmp \
        --canonOnly \
        --primaryOnly \
        --outprefix {params.output_prefix}
        """

# initialize db (uses SQLite)...
# follow up with Denis about additional options...
rule talon_initialize:
    input:
        config["annotation"] #pass in via config
    output:
        "results/talon/{dataset}_gene_models.db" #only one genemodels.db per run...
    conda:
        "envs/talon.yaml"
    params:
        annot = basename(config["annotation"]),
        build = basename(config["reference"]),
        output_prefix = lambda wildcards, output: output[0].split('.')[0]
    shell: 
        """
        talon_initialize_database \
        --f {input} \
        --a {params.annot} \
        --g {params.build} \
        --l 200 \
        --idprefix TALON \
        --5p 1000 \
        --3p 1000 \
        --o {params.output_prefix}
        """

#label/flag reads...
rule talon_label:
    input:
        "results/transcript_clean/{dataset}/{sample}_clean.sam" #aligned, filtered reads
    output:
        "results/talon_label/{dataset}/{sample}_labeled.sam",
        "results/talon_label/{dataset}/{sample}_read_labels.tsv"
    conda:
        "envs/talon.yaml"
    threads: 1
    params:
        output_prefix = lambda wc, output: output[0].split('_labeled.sam')[0],
        ref = config["reference"],
        ar = 20,
        tempdir = "tmp/{sample}/"
    shell:
        """
        talon_label_reads \
        --f {input} \
        --g {params.ref}  \
        --t {threads} \
        --ar {params.ar} \
        --tmpDir {params.tempdir} \
        --deleteTmp \
        --o {params.output_prefix}
        """

#writes a new config.csv... 
#tweak this... 
rule create_talon_config:
    input:
        # all labeled files as input... 
        expand("results/talon_label/{dataset}/{sample}_labeled.sam", zip, sample = samples.index, dataset = samples.dataset)
    params:
        config = config["samples"]
    output:
        "results/talon/{dataset}_config.csv"
    run:
        import pandas as pd
        tbl = pd.read_csv(params.config, sep = ',')
        tbl["file"] = input
        keeps = tbl.dataset == wildcards.dataset
        cols = ["sample", "dataset", "source", "file"]
        tbl.loc[keeps, cols].to_csv(output[0], header = False, index = False)

#TALON... 
rule talon:
    input:
        config = "results/talon/{dataset}_config.csv",
        db = "results/talon/{dataset}_gene_models.db"
    params: 
        build = basename(config["reference"])
    threads:
        10
    output:
        "results/talon/{dataset}_QC.log"
    conda:
        "envs/talon.yaml"
    shell: 
        """
        talon \
        --f {input.config} \
        --db {input.db} \
        --build {params.build} \
        --threads {threads}
        --cov 0.99 \
        --identity 0.95 \
        --o results/{wildcards.dataset}
        """

# transcript model filtering... 
# dataset names need to match talon config...
# this differs from Denis's run... which supplies an SQL query
rule talon_filter:
    input:
        "results/talon/{dataset}_gene_models.db",
        "results/talon/{dataset}_QC.log"
    output:
        "results/talon/{dataset}_filtered_transcripts.csv"
    params:
        samples = lambda wc: ",".join(samples.index[samples.dataset == wc.dataset]),
        maxFracA = 0.5,
        minCount = 5,
        minDatasets = 2,
        annot = basename(config["annotation"])
    conda:
        "envs/talon.yaml"
    shell:
        """
        talon_filter_transcripts \
        --db {input[0]} \
        --datasets {params.samples} \
        -a {params.annot} \
        --maxFracA {params.maxFracA} \
        --minCount {params.minCount} \
        --minDatasets {params.minDatasets} \
        --o {output}
        """

#tabulate
rule talon_abundance:
    input:
        "results/talon/{dataset}_gene_models.db",
        "results/talon/{dataset}_filtered_transcripts.csv"
    output: 
        "results/talon/{dataset}_talon_abundance_filtered.tsv"
    conda:
        "envs/talon.yaml"
    params:
        annot = basename(config["annotation"]),
        build = basename(config["reference"]),
        output_prefix = lambda wc, output: output[0].split("_talon_abundance_filtered.tsv")[0]
    shell:
        """
        talon_abundance \
        --db {input[0]} \
        --whitelist {input[1]} \
        --annot {params.annot} \
        --build {params.build} \
        --o {params.output_prefix}
        """

#GTF
rule talon_GTF:
    input:
        "results/talon/{dataset}_gene_models.db",
        "results/talon/{dataset}_filtered_transcripts.csv"
    output:
        "results/talon/{dataset}_talon.gtf"
    conda:
        "envs/talon.yaml"
    params:
        annot = basename(config["annotation"]),
        build = basename(config["reference"]),
        output_prefix = lambda wc, output: output[0][:-10] 
    shell: 
        """
        talon_create_GTF \
        --db {input[0]} \
        --whitelist {input[1]} \
        -a {params.annot} \
        --build {params.build} \
        --o {params.output_prefix}
        """
